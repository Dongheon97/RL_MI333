{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd97be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/dnclab/env/mi333/lib/python3.8/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08ab84",
   "metadata": {},
   "source": [
    "#### Q-Table\n",
    " - 실제 문제 상황에 적용하기에는 table의 크기가 너무 커짐\n",
    "\n",
    "#### Q-Network\n",
    " - network를 사용해보자\n",
    " - input : state s와 action a\n",
    " - output : Q-Value (모든 가능한 action에 대한 Q-value)\n",
    " - diverge\n",
    " \n",
    "#### Approximate\n",
    " - output : Ws를 optimal한 q-value가 되도록\n",
    " - Linear Regression의 문제가 됨\n",
    " - Ws=Q-prediction, y=r+dis*maxQ(s')\n",
    " - Minimize : (Ws - y)^2\n",
    " \n",
    "#### DQN\n",
    " - Deep, Replay, Seperated networks를 사용해서 converge하게 함\n",
    " - Q-Network Problems:\n",
    "     - 1) Correlations between samples : 정답과 거리가 생긴다\n",
    "     - 2) Non-stationary targets : 타겟이 움직인다\n",
    " - DQN Solutions:\n",
    "     - 1) Go Deep\n",
    "     - 2) Capture & Replay (P1)\n",
    "         Experience Replay: 행동을 통한 상태를 버퍼에 저장한다(minibatch) -> 일정한 시간이 지나면 랜덤하게 뽑아 학습한다 -> 랜덤하면 전체의 분포와 비슷하게 나올 수 있다(조금씩 바뀌면서 학습이 진행된다\n",
    "     - 3) Seperate networks : create a target network (P2)\n",
    "\n",
    " - theta- = theta\n",
    " - R_t + gamma * maxQ(S_t+1, a_t+1)\n",
    "\n",
    "`\n",
    "q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "next_q_value     = next_q_values.max(1)[0]\n",
    "expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "`         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba6b9c",
   "metadata": {},
   "source": [
    "#### Double Q-Learning (DQN)\n",
    " - Q-Learning's over-estimate problem\n",
    "     - 에이전트가 max Q value를 가지고 있기 때문에 주어진 상태에서 항상 최적이 아닌 동작을 선택\n",
    "     - q-value가 noise를 가지게 됨\n",
    "     - 추정된 q-value으로부터의 잡음은 업데이트 과정에서 큰 편향을 유발 -> 학습과정 복잡\n",
    " - Q-Network와 Target Q Network를 분리\n",
    "     - Action : Online Q-Network (Selection)\n",
    "     - Action-value : Target Q-Network (Evaluation)\n",
    " \n",
    " - R_t + gamma * Q_w-(S_t+1, argmax Q_w(S_t+1, a_t+1))\n",
    " - Main Net을 maximize 하는 weights 를 target Net에 넣는다.\n",
    "     \n",
    "`\n",
    "q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1) \n",
    "next_q_value     = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    " `\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942229a2",
   "metadata": {},
   "source": [
    "#### Dueling DQN\n",
    " - Advantage Function (Q-V) 과 Value Function을 따로 구함 -> 이들을 합쳐 Q(s, a)를 구한다\n",
    " - 두 function의 estimator가 동시에 존재하여 Q 추정 -> Dueling\n",
    " - Forward 단계에서는 큰 차이 없다 -> But) Backpropagation 단계에서 두가지 정보를 나눠서 주는 차이 있다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f550c3",
   "metadata": {},
   "source": [
    "#### Actor-Critic\n",
    " - Actor : Update Policy, P_theta(a_t|S_t)\n",
    " - Critic : Update Q_w \n",
    " - evaluates Actor's action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2a4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58e715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a5987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a5c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
